# ğŸ¥ YouTube Data Collection & Sentiment Analysis

This project demonstrates how to collect YouTube video data using the `tuber` package in R, analyze viewer comments, generate a word cloud, and perform sentiment analysis.

---

## ğŸ“Š Features
âœ… Authenticates with the YouTube Data API (v3)  
âœ… Collects video statistics, details, and comments  .
âœ… Cleans and processes comments for text analysis .
âœ… Generates a *word cloud of frequent terms .
âœ… Performs sentiment analysis with NRC lexicon  
âœ… Visualizes emotional tone of viewer comments  

---

## ğŸ“‚ Files in This Repo
- youtube_analytics.Rmdâ€“ Clean, portfolioâ€‘ready R Markdown  
- README.md â€“ This project overview and instructions  
 


---

## ğŸš€ How to Run
1ï¸âƒ£ Clone this repo or download it as a ZIP.  
2ï¸âƒ£ Install required packages in R:  
```r
install.packages(c("tuber", "tm", "SnowballC", "wordcloud", "syuzhet"))
```
3ï¸âƒ£ Get your YouTube API credentials from Google Cloud Console.  
4ï¸âƒ£ Open `youtube_analytics.Rmd` in **RStudio** and replace placeholders:
```r
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"
```
5ï¸âƒ£ Knit the R Markdown file to render the **HTML report**.

---


---

## ğŸ”‘ Authentication Setup

This project uses the **YouTube Data API (v3)** via the `tuber` R package.

When you run this code for the first time:

1ï¸âƒ£ You will see a browser window asking you to log in to your Google/YouTube account.  
2ï¸âƒ£ R will prompt you with:  
   Use a local file (.httr-oauth) to cache OAuth access credentials between R sessions?  
   âœ… **Type 1 (Yes)** to save your login.  
3ï¸âƒ£ This will create a **.httr-oauth** file in your project directory.  

**Important:**  
 
- Anyone else cloning this repo must authenticate with **their own** YouTube API credentials.
